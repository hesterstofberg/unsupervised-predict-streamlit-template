{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MovieLens Dataset\n",
    "One of the most common datasets that is available on the internet for building a Recommender System is the [MovieLens DataSet](https://grouplens.org/datasets/movielens/). This version of the dataset that I'm working with ([1M](https://grouplens.org/datasets/movielens/1m/)) contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000.\n",
    "\n",
    "The data was collected by GroupLens researchers over various periods of time, depending on the size of the set. This 1M version was released on February 2003. Users were selected at random for inclusion. All users selected had rated at least 20 movies. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The original data are contained in three files, [movies.dat](https://github.com/khanhnamle1994/movielens/blob/master/dat/movies.dat), [ratings.dat](https://github.com/khanhnamle1994/movielens/blob/master/dat/ratings.dat) and [users.dat](https://github.com/khanhnamle1994/movielens/blob/master/dat/users.dat). To make it easier to work with the data, I converted them into csv files. The process can be viewed in my [Data Processing Notebook](https://github.com/khanhnamle1994/movielens/blob/master/Data_Processing.ipynb).\n",
    "\n",
    "![movielens](images/movielens.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's load this data into Python. I will load the dataset with Pandas onto Dataframes **ratings**, **users**, and **movies**. Before that, I'll also pass in column names for each CSV and read them using pandas (the column names are available in the [Readme](https://github.com/khanhnamle1994/movielens/blob/master/README.md) file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('kaggle/train.csv')\n",
    "\n",
    "# Reading users file\n",
    "#users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('kaggle/movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a peak into the content of each file to understand them better.\n",
    "\n",
    "### Ratings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5163</td>\n",
       "      <td>57669</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1518349992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106343</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1206238739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146790</td>\n",
       "      <td>5459</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1076215539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106362</td>\n",
       "      <td>32296</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1423042565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9041</td>\n",
       "      <td>366</td>\n",
       "      <td>3.0</td>\n",
       "      <td>833375837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0    5163    57669     4.0  1518349992\n",
       "1  106343        5     4.5  1206238739\n",
       "2  146790     5459     5.0  1076215539\n",
       "3  106362    32296     2.0  1423042565\n",
       "4    9041      366     3.0   833375837"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000038 entries, 0 to 10000037\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   userId     int64  \n",
      " 1   movieId    int64  \n",
      " 2   rating     float64\n",
      " 3   timestamp  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 305.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Movies Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62423 entries, 0 to 62422\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   movieId  62423 non-null  int64 \n",
      " 1   title    62423 non-null  object\n",
      " 2   genres   62423 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains attributes of the 3883 movies. There are 3 columns including the movie ID, their titles, and their genres. Genres are pipe-separated and are selected from 18 genres (Action, Adventure, Animation, Children's, Comedy, Crime, Documentary, Drama, Fantasy, Film-Noir, Horror, Musical, Mystery, Romance, Sci-Fi, Thriller, War, Western)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Recommendation Model\n",
    "The content based engine suffers from some severe limitations. It is only capable of suggesting movies which are close to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres.\n",
    "\n",
    "Also, the engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who she/he is.\n",
    "\n",
    "Therefore, in this section, I will use Memory-Based Collaborative Filtering to make recommendations to movie users. The technique is based on the idea that users similar to a me can be used to predict how much I will like a particular product or service those users have used/experienced but I have not.\n",
    "\n",
    "### Theory\n",
    "There are 2 main types of memory-based collaborative filtering algorithms:\n",
    "1. **User-User Collaborative Filtering**: Here we find look alike users based on similarity and recommend movies which first user’s look-alike has chosen in past. This algorithm is very effective but takes a lot of time and resources. It requires to compute every user pair information which takes time. Therefore, for big base platforms, this algorithm is hard to implement without a very strong parallelizable system.\n",
    "2. **Item-Item Collaborative Filtering**: It is quite similar to previous algorithm, but instead of finding user's look-alike, we try finding movie's look-alike. Once we have movie's look-alike matrix, we can easily recommend alike movies to user who have rated any movie from the dataset. This algorithm is far less resource consuming than user-user collaborative filtering. Hence, for a new user, the algorithm takes far lesser time than user-user collaborate as we don’t need all similarity scores between users. And with fixed number of movies, movie-movie look alike matrix is fixed over time.\n",
    "\n",
    "![user_item_cf](images/user_item_cf.jpg)\n",
    "\n",
    "In either scenario, we builds a similarity matrix. For user-user collaborative filtering, the **user-similarity matrix** will consist of some distance metrics that measure the similarity between any two pairs of users. Likewise, the **item-similarity matrix** will measure the similarity between any two pairs of items.\n",
    "\n",
    "There are 3 distance similarity metrics that are usually used in collaborative filtering:\n",
    "1. **Jaccard Similarity**:\n",
    "    * Similarity is based on the number of users which have rated item A and B divided by the number of users who have rated either A or B\n",
    "    * It is typically used where we don’t have a numeric rating but just a boolean value like a product being bought or an add being clicked\n",
    "\n",
    "2. **Cosine Similarity**: (as in the Content-Based system)\n",
    "    * Similarity is the cosine of the angle between the 2 vectors of the item vectors of A and B\n",
    "    * Closer the vectors, smaller will be the angle and larger the cosine\n",
    "\n",
    "3. **Pearson Similarity**:\n",
    "    * Similarity is the pearson coefficient between the two vectors.\n",
    "\n",
    "For the purpose of diversity, I will use **Pearson Similarity** in this implementation.\n",
    "\n",
    "### Implementation\n",
    "I will use the file **ratings.csv** first as it contains User ID, Movie IDs and Ratings. These three elements are all I need for determining the similarity of the users based on their ratings for a particular movie.\n",
    "\n",
    "First I do some quick data processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "ratings['userId'] = ratings['userId'].fillna(0)\n",
    "ratings['movieId'] = ratings['movieId'].fillna(0)\n",
    "\n",
    "# Replace NaN values in rating column with average of all values\n",
    "ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited computing power in my laptop, I will build the recommender system using only a subset of the ratings. In particular, I will take a random sample of 20,000 ratings (2%) from the 1M ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20000 entries, 4043101 to 592746\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   userId     20000 non-null  int64  \n",
      " 1   movieId    20000 non-null  int64  \n",
      " 2   rating     20000 non-null  float64\n",
      " 3   timestamp  20000 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 781.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 1% of the ratings dataset\n",
    "train_data = ratings.sample(frac=0.002)\n",
    "# Check the sample info\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I use the **scikit-learn library** to split the dataset into testing and training.  **Cross_validation.train_test_split** shuffles and splits the data into two datasets according to the percentage of test examples, which in this case is 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#train_data, test_data = train_test_split(small_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4043101</th>\n",
       "      <td>1831</td>\n",
       "      <td>81564</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8734718</th>\n",
       "      <td>133495</td>\n",
       "      <td>4262</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231575</th>\n",
       "      <td>54125</td>\n",
       "      <td>260</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860604</th>\n",
       "      <td>96602</td>\n",
       "      <td>2401</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687048</th>\n",
       "      <td>85805</td>\n",
       "      <td>203417</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525179</th>\n",
       "      <td>129561</td>\n",
       "      <td>2677</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061818</th>\n",
       "      <td>47145</td>\n",
       "      <td>5147</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3502110</th>\n",
       "      <td>140279</td>\n",
       "      <td>1193</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7817966</th>\n",
       "      <td>90243</td>\n",
       "      <td>2115</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592746</th>\n",
       "      <td>6609</td>\n",
       "      <td>2997</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  rating\n",
       "4043101    1831    81564     3.5\n",
       "8734718  133495     4262     3.0\n",
       "3231575   54125      260     5.0\n",
       "5860604   96602     2401     3.0\n",
       "687048    85805   203417     3.0\n",
       "...         ...      ...     ...\n",
       "525179   129561     2677     5.0\n",
       "2061818   47145     5147     5.0\n",
       "3502110  140279     1193     5.0\n",
       "7817966   90243     2115     3.0\n",
       "592746     6609     2997     5.0\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.drop('timestamp', axis=1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = test_data.drop('timestamp', axis=1)\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to create a user-item matrix. Since I have splitted the data into testing and training, I need to create two matrices. The training matrix contains 80% of the ratings and the testing matrix contains 20% of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = train_data.to_numpy()\n",
    "#test_data_matrix = test_data.to_numpy()\n",
    "\n",
    "# Check their shape\n",
    "print(train_data_matrix.shape)\n",
    "#print(test_data_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I use the **pairwise_distances** function from sklearn to calculate the [Pearson Correlation Coefficient](https://stackoverflow.com/questions/1838806/euclidean-distance-vs-pearson-correlation-vs-cosine-similarity). This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(train_data, metric='correlation')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.01081297  0.00278602]\n",
      " [ 0.01081297  1.         -0.01683925]\n",
      " [ 0.00278602 -0.01683925  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(train_data_matrix.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "print(item_correlation[:4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the similarity matrix in hand, I can now predict the ratings that were not included with the data. Using these predictions, I can then compare them with the test data to attempt to validate the quality of our recommender model.\n",
    "\n",
    "For the user-user CF case, I will look at the similarity between 2 users (A and B, for example) as weights that are multiplied by the ratings of a similar user B (corrected for the average rating of that user). I also need to normalize it so that the ratings stay between 1 and 5 and, as a final step, sum the average ratings for the user that I am trying to predict. The idea here is that some users may tend always to give high or low ratings to all movies. The relative difference in the ratings that these users give is more important than the absolute values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict ratings\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_user_rating = train_data_matrix.mean(axis=1)\n",
    "# Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "ratings_diff = (train_data_matrix - mean_user_rating[:, np.newaxis])\n",
    "pred = mean_user_rating[:, np.newaxis] + user_correlation.dot(ratings_diff) / np.array([np.abs(user_correlation).sum(axis=1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 69786.42316418,  -5520.66640765, -15080.75675653],\n",
       "       [ 60310.58052289, -13185.92786412, -25072.65265876],\n",
       "       [ 79227.39915572,  71217.96485854,  10937.63598574],\n",
       "       ...,\n",
       "       [119182.8998016 ,  59356.49471635,  36445.10548205],\n",
       "       [ 98180.3071366 ,  22413.60546922,  13530.08739419],\n",
       "       [ 74786.4635389 ,    935.21253925, -10532.67607815]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = train_data_matrix.dot(item_correlation) / np.array([np.abs(item_correlation).sum(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.64571111e+04,  2.55325262e+03,  9.75962854e+01],\n",
       "       [ 1.84175077e+04,  3.52201437e+03, -2.59926055e+00],\n",
       "       [ 5.77381631e+04,  1.01785979e+05, -1.55648379e+03],\n",
       "       ...,\n",
       "       [ 1.23024920e+05,  9.01186933e+04, -1.16886825e+03],\n",
       "       [ 1.32034666e+05,  1.69614774e+03,  3.61760147e+02],\n",
       "       [ 5.55681948e+04,  9.30752286e+03,  1.06119533e+01]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('kaggle/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId\n",
       "0       1     2011\n",
       "1       1     4144\n",
       "2       1     5767\n",
       "3       1     6711\n",
       "4       1     7318"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000019 entries, 0 to 5000018\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype\n",
      "---  ------   -----\n",
      " 0   userId   int64\n",
      " 1   movieId  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 76.3 MB\n"
     ]
    }
   ],
   "source": [
    "testdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3abc93a74d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fill NaN values in user_id and movie_id column with 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtestdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtestdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testdf' is not defined"
     ]
    }
   ],
   "source": [
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "testdf['userId'] = testdf['userId'].fillna(0)\n",
    "testdf['movieId'] = testdf['movieId'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,   2011],\n",
       "       [     1,   4144],\n",
       "       [     1,   5767],\n",
       "       ...,\n",
       "       [162541,   4980],\n",
       "       [162541,   5689],\n",
       "       [162541,   7153]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = testdf.to_numpy()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings on the training data with both similarity score\n",
    "user_prediction = predict(train_data_matrix, user_correlation, type='user')\n",
    "item_prediction = predict(train_data_matrix, item_correlation, type='item')\n",
    "\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "There are many evaluation metrics but one of the most popular metric used to evaluate accuracy of predicted ratings is **Root Mean Squared Error (RMSE)**. I will use the **mean_square_error (MSE)** function from sklearn, where the RMSE is just the square root of MSE.\n",
    "\n",
    "$$\\mathit{RMSE} =\\sqrt{\\frac{1}{N} \\sum (x_i -\\hat{x_i})^2}$$\n",
    "\n",
    "I'll use the scikit-learn's **mean squared error** function as my validation metric. Comparing user- and item-based collaborative filtering, it looks like user-based collaborative filtering gives a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(actual, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 43534.87219441658\n",
      "Item-based CF RMSE: 49804.268660602385\n"
     ]
    }
   ],
   "source": [
    "# Predict ratings on the training data with both similarity score\n",
    "user_prediction = predict(train_data_matrix, user_correlation, type='user')\n",
    "item_prediction = predict(train_data_matrix, item_correlation, type='item')\n",
    "\n",
    "# RMSE on the test data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 23802.91797386833\n",
      "Item-based CF RMSE: 1042.937024665936\n"
     ]
    }
   ],
   "source": [
    "# RMSE on the train data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, train_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, train_data_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE of training of model is a metric which measure how much the signal and the noise is explained by the model. I noticed that my RMSE is quite big. I suppose I might have overfitted the training data.\n",
    "\n",
    "Overall, Memory-based Collaborative Filtering is easy to implement and produce reasonable prediction quality. However, there are some drawback of this approach:\n",
    "\n",
    "* It doesn't address the well-known cold-start problem, that is when new user or new item enters the system. \n",
    "* It can't deal with sparse data, meaning it's hard to find users that have rated the same items.\n",
    "* It suffers when new users or items that don't have any ratings enter the system.\n",
    "* It tends to recommend popular items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Approach\n",
    "As I mentioned above, it looks like my Collaborative Filtering model suffers from overfitting problem as I only train it on a small sample dataset (2% of the actual 1M ratings). In order to deal with this, I need to apply dimensionality reduction techniques to capture more signals from the big dataset. Thus comes the use of **low-dimensional factor models (aka, Model-Based Collaborative Filtering)**. I won't be able to implement this approach in this notebook due to computing limit, however, I want to introduce it here to give you a general sense of its advantages.\n",
    "\n",
    "In this approach, CF models are developed using machine learning algorithms to predict user’s rating of unrated items. It has been shown that Model-based Collaborative Filtering has received greater exposure in industry research, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. An example is the competition to win the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize), which used the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films.\n",
    "\n",
    "Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF. The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items. As per my understanding, the algorithms in this approach can further be broken down into 3 sub-types:\n",
    "\n",
    "* **Matrix Factorization (MF)**: The idea behind such models is that attitudes or preferences of a user can be determined by a small number of hidden latent factors. These factors are also called **Embeddings**, which represent different characteristics for users and items. Matrix factorization can be done by various methods including Support Vecot Decomposition (SVD), Probabilistic Matrix Factorization (PMF), and Non-Negative Matrix Factorization (NMF).\n",
    "\n",
    "* **Clustering based algorithm (KNN)**: The idea of clustering is same as that of memory-based recommendation systems. In memory-based algorithms, we use the similarities between users and/or items and use them as weights to predict a rating for a user and an item. The difference is that the similarities in this approach are calculated based on an unsupervised learning model, rather than Pearson correlation or cosine similarity.\n",
    "\n",
    "* **Neural Nets / Deep Learning**: The idea of using Neural Nets is similar to that of Model-Based Matrix Factorization. In matrix factorizaion, we decompose our original sparse matrix into product of 2 low rank orthogonal matrices. For neural net implementation, we don’t need them to be orthogonal, we want our model to learn the values of embedding matrix itself. The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n",
    "\n",
    "![memory-model-cf](images/memory-model-cf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this post, I introduced the Movie Lens dataset for building movie recommendation system.\n",
    "\n",
    "Specifically, I have developed recommendation models including:\n",
    "\n",
    "* How to load and review the data.\n",
    "* How to develop a content-based recommendation model based on movie genres.\n",
    "* How to develop a memory-based collaborative filtering model based on user ratings.\n",
    "* A glimpse at model-based collaborative filtering models as alternative options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
